{"paragraphs":[{"title":"Spark & CHO Data - Spark Workshop, Code4Lib 2018","text":"%md\nSpark Workshop, code4lib 2018\nMaterials at [github.com/spark4lib/code4lib2018](https://github.com/spark4lib/code4lib2018)\n\nThe following steps should also be available, with other notes, here: [https://github.com/spark4lib/code4lib2018/tree/master/worksheets/](https://github.com/spark4lib/code4lib2018/tree/master/worksheets) .","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Spark Workshop, code4lib 2018<br/>Materials at <a href=\"https://github.com/spark4lib/code4lib2018\">github.com/spark4lib/code4lib2018</a></p>\n<p>The following steps should also be available, with other notes, here: <a href=\"https://github.com/spark4lib/code4lib2018/tree/master/worksheets\">https://github.com/spark4lib/code4lib2018/tree/master/worksheets/</a> .</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784089_991338542","id":"20180131-144721_1911570621","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:456"},{"title":"Introduction","text":"%md\nIn this session, we want you to apply what we learned in Spark Shell Introduction (Spark SQL, PySpark, & Zeppelin) for analyzing some provided Cultural Heritage Organization (CHO) data. \n\nYou will be working in small groups on this.","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this session, we want you to apply what we learned in Spark Shell Introduction (Spark SQL, PySpark, &amp; Zeppelin) for analyzing some provided Cultural Heritage Organization (CHO) data. </p>\n<p>You will be working in small groups on this.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784097_975948586","id":"20180211-041642_2058564180","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:457"},{"title":"Environment Set-up Test","text":"%spark\n\nprintln(\"Spark Version: \" + sc.version)\n\nval penndata = sc.textFile(\"penn.csv\")\nval cmoadata = sc.textFile(\"cmoa.csv\")\nprintln(\"penn count: \" + penndata.count)\nprintln(\"cmoadata count: \" + cmoadata.count)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784097_975948586","id":"20180131-144538_432654498","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:458"},{"title":"Preview CHO Data Options: CMOA","text":"%sh\n\ncat code4lib2018/sample-data/cmoa/README\nhead code4lib2018/sample-data/cmoa/cmoa.csv","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784098_977102833","id":"20180210-010216_1168213332","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:459"},{"title":"Preview CHO Data Options: Penn","text":"%sh\n\ncat code4lib2018/sample-data/penn\\ museum/README\nunzip code4lib2018/sample-data/penn\\ museum/all-20180121.zip -d code4lib2018/sample-data/penn\\ museum/\nhead code4lib2018/sample-data/penn\\ museum/all-20180121.csv","dateUpdated":"2018-02-12T06:26:59+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784098_977102833","id":"20180212-044729_2120047837","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:460","user":"anonymous","dateFinished":"2018-02-12T06:27:01+0000","dateStarted":"2018-02-12T06:27:00+0000"},{"title":"","text":"%md\n# Part 1: Creating a Dataframe Instance from CHO CSV","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Part 1: Creating a Dataframe Instance from CHO CSV</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784098_977102833","id":"20180211-043612_1888524633","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:461"},{"title":"Read a CSV File into a Dataframe Instance with Headers","text":"pennDf = spark.read.csv(\"code4lib2018/sample-data/penn\\ museum/all-20180121.csv\", header=True, inferSchema=True)\npennDf.printSchema()","dateUpdated":"2018-02-12T06:27:03+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784099_976718084","id":"20180210-002750_1470779955","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:462","user":"anonymous","dateFinished":"2018-02-12T06:27:07+0000","dateStarted":"2018-02-12T06:27:04+0000"},{"title":"Read a CSV File into a Dataframe Instance with Headers","text":"pennDf.show(10, truncate=True)","dateUpdated":"2018-02-12T06:27:09+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784099_976718084","id":"20180211-233520_399031330","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:463","user":"anonymous","dateFinished":"2018-02-12T06:27:10+0000","dateStarted":"2018-02-12T06:27:10+0000"},{"title":"Show Columns","text":"pennDf.columns","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784099_976718084","id":"20180212-045044_481169447","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:464"},{"title":"Check out emuIRN","text":"pennDf.select('emuIRN').distinct().show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":4,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784099_976718084","id":"20180211-234002_1351508361","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:465"},{"title":"Check out Object Number","text":"pennDf.select('object_number').distinct().show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":4,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784100_974794339","id":"20180211-234030_834783507","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:466"},{"title":"Check out url","text":"pennDf.select('url').distinct().show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":4,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784100_974794339","id":"20180211-234038_1787924327","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:467"},{"title":"Create Our Own Schema & Read the CSV into a Dataframe Instance with our Schema","text":"from pyspark.sql.types import *\n\npennSchema = StructType([\n  StructField(\"emuIRN\", IntegerType(), False),\n  StructField(\"curatorial_section\", StringType(), True),\n  StructField(\"object_number\", StringType(), True),\n  StructField(\"object_name\", StringType(), True),\n  StructField(\"native_name\", StringType(), True),\n  StructField(\"culture\", StringType(), True),\n  StructField(\"provenience\", StringType(), True),\n  StructField(\"material\", StringType(), True),\n  StructField(\"period\", StringType(), True),\n  StructField(\"date_made\", StringType(), True),\n  StructField(\"date_made_early\", StringType(), True),\n  StructField(\"date_made_late\", StringType(), True),\n  StructField(\"accession_credit_line\", StringType(), True),\n  StructField(\"creator\", StringType(), True),\n  StructField(\"description\", StringType(), True),\n  StructField(\"manufacture_locationlocus\", StringType(), True),\n  StructField(\"culture_area\", StringType(), True),\n  StructField(\"technique\", StringType(), True),\n  StructField(\"iconography\", StringType(), True),\n  StructField(\"measurement_height\", StringType(), True),\n  StructField(\"measurement_length\", StringType(), True),\n  StructField(\"measurement_width\", StringType(), True),\n  StructField(\"measurement_outside_diameter\", StringType(), True),\n  StructField(\"measurement_tickness\", StringType(), True),\n  StructField(\"measurement_unit\", StringType(), True),\n  StructField(\"other_numbers\", StringType(), True),\n  StructField(\"url\", StringType(), True)])\n\n\npennDf = spark.read.csv(\"code4lib2018/sample-data/penn\\ museum/all-20180121.csv\", header=True, schema=pennSchema)\npennDf.count()","dateUpdated":"2018-02-12T06:27:17+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784101_974409591","id":"20180205-224139_1405421836","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:468","user":"anonymous","dateFinished":"2018-02-12T06:27:19+0000","dateStarted":"2018-02-12T06:27:17+0000"},{"title":"Get Sample Response of our DataFrame","text":"pennDf.sample(False, 0.10, 42).show()","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784101_974409591","id":"20180212-045408_436964083","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:469"},{"title":"","text":"%md\n### Stopping Point: Now you need to walk through, quickly assess, select, and load a CHO CSV into a Dataframe.","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Stopping Point: Now you need to walk through, quickly assess, select, and load a CHO CSV into a Dataframe.</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784101_974409591","id":"20180211-204040_1741647870","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:470"},{"title":"Loading CSV into DataFrame Working Space","text":"\n","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784101_974409591","id":"20180212-045445_2022795863","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:471"},{"title":"","text":"%md\n# Part 2: Simple Analysis of our Dataset via Dataframe API","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Part 2: Simple Analysis of our Dataset via Dataframe API</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784102_975563837","id":"20180211-044238_542100528","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:472"},{"title":"Show Specific Columns","text":"pennDf.select(\"curatorial_section\", \"emuIRN\").show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784102_975563837","id":"20180211-044743_1705262796","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:473"},{"title":"Filter Specific Columns ","text":"americanDf = pennDf.filter(pennDf.curatorial_section == 'American')\namericanDf.select('emuIRN', 'url', 'object_name', 'creator').show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784102_975563837","id":"20180210-011244_1927422767","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:474"},{"title":"\"Earliest\" Dates for Objects?","text":"pennDf.orderBy(\"date_made_early\").select('emuIRN', 'date_made', 'object_name', 'object_number', 'culture_area').show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784102_975563837","id":"20180210-212920_1413184855","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:475"},{"title":"\"Earliest\" , Non-Null Dates for Objects?","text":"pennDf.filter(pennDf.date_made.isNotNull()).orderBy(\"date_made_early\").select('emuIRN', 'date_made', 'object_name', 'object_number', 'culture_area').show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784103_975179088","id":"20180212-050343_962594735","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:476"},{"title":"How Many Items per Curatorial Section?","text":"pennDf.groupBy(\"curatorial_section\").count().show()","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784103_975179088","id":"20180210-011435_1074589917","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:477"},{"title":"Average Height of Objects by Period?","text":"pennDf.groupBy(\"period\").agg({\"measurement_height\": 'mean'}).show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784103_975179088","id":"20180211-053908_436020978","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:478"},{"title":"Distinct Creators","text":"pennDf.select(\"creator\").distinct().show()","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":4,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784103_975179088","id":"20180210-211959_911280658","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:479"},{"title":"Number of Distinct Creators","text":"pennDf.select(\"creator\").distinct().count()","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":4,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784103_975179088","id":"20180212-051332_1623401677","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:480"},{"title":"Number of Related Resources per Creator","text":"from pyspark.sql.functions import countDistinct\npennDf.select(\"creator\", \"emuIRN\")\\\n      .groupBy(\"creator\")\\\n      .agg(countDistinct(\"emuIRN\")\\\n      .alias(\"objects\"))\\\n      .orderBy(\"objects\", ascending=False).show(10)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":4,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784103_975179088","id":"20180212-051343_572426650","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:481"},{"title":"","text":"%md\n### Stopping Point: Use Grouping, Distinct, Ordering, etc. To Get Analytics on Your CHO Dataset.","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Stopping Point: Use Grouping, Distinct, Ordering, etc. To Get Analytics on Your CHO Dataset.</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784104_973255344","id":"20180211-204432_1327378340","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:482"},{"title":"Your Working Space: Analysis of Data","text":"\n","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784104_973255344","id":"20180211-204542_862151313","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:483"},{"title":"","text":"%md\n# Part 3: Creating Derivative DataFrames for your CHO","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Part 3: Creating Derivative DataFrames for your CHO</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784104_973255344","id":"20180211-052933_847596982","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:484"},{"title":"Get DataFrame with Curatorial, ID, Culture, URL, & Period, Dropping Nulls","text":"curatorialOnlyDf = pennDf.select('curatorial_section', 'emuIRN', 'culture', 'url', 'period').dropna()\ncuratorialOnlyDf.sample(False, 0.01, 42).show()","dateUpdated":"2018-02-12T06:27:29+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784104_973255344","id":"20180210-212150_723012495","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:485","user":"anonymous","dateFinished":"2018-02-12T06:27:30+0000","dateStarted":"2018-02-12T06:27:29+0000"},{"title":"Get DataFrame with Records that Have no Title, Creator or Description","text":"noDescDf = pennDf.filter(pennDf.creator.isNull()).filter(pennDf.description.isNull()).filter(pennDf.object_name.isNull())\nprint(noDescDf.count())\nnoDescDf.select('description', 'creator', 'emuIRN', 'curatorial_section', 'object_name').show(10, truncate=True)","dateUpdated":"2018-02-12T06:27:32+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784104_973255344","id":"20180211-032722_1642010293","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:486","user":"anonymous","dateFinished":"2018-02-12T06:27:33+0000","dateStarted":"2018-02-12T06:27:32+0000"},{"title":"Explode Object Names based on '|' Delimiter","text":"# Note the requirement for the escape character for |\nfrom pyspark.sql.functions import explode\n\nexplodedNamesDf = pennDf.withColumn(\"object_name\", explode(split(\"object_name\", \"\\|\")))\nexplodedNamesDf.show(20)","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784105_972870595","id":"20180210-220422_1187559654","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:487"},{"title":"Create Columns for AD or for BC","text":"pennDf = pennDf.withColumn('AD', pennDf.date_made.like(\"%AD%\"))\npennDf = pennDf.withColumn('BC', pennDf.date_made.like(\"%BC%\"))\npennDf.select('emuIRN', 'AD', 'BC', 'date_made').show()","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{"0":{"graph":{"mode":"table","height":478,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784105_972870595","id":"20180210-213550_325903017","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:488"},{"title":"Remove Whitespace around Dates (Bc Why Not)","text":"from pyspark.sql.functions import regexp_replace\n\ndateTestDf = pennDf.withColumn(\"date_made_cleaned\", regexp_replace(pennDf.date_made, \"\\s+\", \"\"))\ndateTestDf.select(\"date_made_cleaned\").distinct().show()","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784105_972870595","id":"20180211-035409_418875897","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:489"},{"title":"","text":"%md\n### Stopping Point: Create some Derivative Dataframes with your CHO dataset. Think of what would make a good SQL Query dataset.","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Stopping Point: Create some Derivative Dataframes with your CHO dataset. Think of what would make a good SQL Query dataset.</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784105_972870595","id":"20180211-205149_2050933534","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:490"},{"title":"Your Working Space: Deriving DataFrames","text":"\n","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784105_972870595","id":"20180211-205238_1685626655","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:491"},{"title":"","text":"%md\n# Part 4: Using SQL to Analyze our Dataset","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Part 4: Using SQL to Analyze our Dataset</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784106_974024842","id":"20180211-070011_2131671710","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:492"},{"title":"Introduction to SQL Queries ","text":"%md\n[Here](http://cse.unl.edu/~sscott/ShowFiles/SQL/CheatSheet/SQLCheatSheet.html) is a SQL Cheatsheet in case you need it.","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><a href=\"http://cse.unl.edu/~sscott/ShowFiles/SQL/CheatSheet/SQLCheatSheet.html\">Here</a> is a SQL Cheatsheet in case you need it.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784106_974024842","id":"20180211-040724_834116241","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:493"},{"title":"Creating a Temporary SQL View from Penn DataFrames","text":"curatorialOnlyDf.createOrReplaceTempView(\"curatorialView\")\nnoDescDf.createOrReplaceTempView(\"noDescView\")","dateUpdated":"2018-02-12T06:27:43+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784106_974024842","id":"20180211-071952_1262010614","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:494","user":"anonymous","dateFinished":"2018-02-12T06:27:43+0000","dateStarted":"2018-02-12T06:27:43+0000"},{"title":"Query Penn DataFrame View for Number of Cultures per Curatorial Section","text":"sparkQuery = spark.sql(\"\"\"SELECT curatorial_section, COUNT(DISTINCT(culture)) AS NumCultures FROM curatorialView GROUP BY curatorial_section\"\"\")\nfor n in sparkQuery.collect():\n    n","dateUpdated":"2018-02-12T06:30:38+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784106_974024842","id":"20180211-072058_265727285","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:495","user":"anonymous","dateFinished":"2018-02-12T06:29:53+0000","dateStarted":"2018-02-12T06:29:50+0000"},{"title":"Query Penn DataFrame View for Number of No Title-Creator-Description Records per Curatorial Section","text":"sparkQuery2 = spark.sql(\"\"\"SELECT curatorial_section, COUNT(DISTINCT(emuIRN)) AS NumResources FROM noDescView GROUP BY curatorial_section\"\"\")\nfor n in sparkQuery.collect():\n    n","dateUpdated":"2018-02-12T06:30:27+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784106_974024842","id":"20180212-061229_887304088","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:496","user":"anonymous","dateFinished":"2018-02-12T06:30:23+0000","dateStarted":"2018-02-12T06:30:21+0000"},{"title":"","text":"%md\n### Stopping Point: Create a Temporary View & Run a SQL Query","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Stopping Point: Create a Temporary View &amp; Run a SQL Query</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784106_974024842","id":"20180211-205631_117220376","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:497"},{"title":"Your Working Space: Create a Spark View from your Dataframe(s)","text":"\n","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784106_974024842","id":"20180211-205653_111316135","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:498"},{"title":"Your Working Space: Run some Spark SQL Queries","text":"\n","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784107_973640093","id":"20180211-211110_718731458","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:499"},{"title":"","text":"%md\n# Part 5: Simple Data Visualization","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Part 5: Simple Data Visualization</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784107_973640093","id":"20180211-072644_1534169002","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:500"},{"text":"%sql \nSELECT curatorial_section, \nCOUNT(DISTINCT(culture)) AS NumCultures \nFROM curatorialView \nGROUP BY curatorial_section","dateUpdated":"2018-02-12T06:31:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":true},"helium":{}}},"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784107_973640093","id":"20180211-210732_864456388","dateCreated":"2018-02-12T06:26:24+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:501","user":"anonymous","dateFinished":"2018-02-12T06:31:40+0000","dateStarted":"2018-02-12T06:31:38+0000"},{"title":"","text":"%md\n### Stopping Point: From your Temporary Views, Create a Simple Viz","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Stopping Point: From your Temporary Views, Create a Simple Viz</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1518416784107_973640093","id":"20180211-211201_433698257","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:502"},{"title":"Your Working Space: Spark View => SQL Query & Viz","text":"%sql\n\n","dateUpdated":"2018-02-12T06:26:24+0000","config":{"lineNumbers":true,"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784107_973640093","id":"20180211-211029_1012657635","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:503"},{"text":"%md\n","dateUpdated":"2018-02-12T06:26:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518416784107_973640093","id":"20180211-080404_990095708","dateCreated":"2018-02-12T06:26:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:504"}],"name":"Spark Workshop: CHO Data","id":"2D6RYGTKS","angularObjects":{"2D8F6TC39:shared_process":[],"2D7NXVY4B:shared_process":[],"2D8A6MQRC:shared_process":[],"2D5PJ3NSA:shared_process":[],"2D6EP7ZA2:shared_process":[],"2D5TJGRY4:shared_process":[],"2D5VZHKP9:shared_process":[],"2D89YMPRJ:shared_process":[],"2D72HVDPJ:shared_process":[],"2D6DB2GSE:shared_process":[],"2D7ZKW5Z8:shared_process":[],"2D6YB5W7T:shared_process":[],"2D6DKU8MK:shared_process":[],"2D4ZW7PCP:shared_process":[],"2D566ZBPK:shared_process":[],"2D866A3V3:shared_process":[],"2D7TTRTEG:shared_process":[],"2D71TQHNV:shared_process":[],"2D57SSSRY:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}