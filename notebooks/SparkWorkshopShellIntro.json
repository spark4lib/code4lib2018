{"paragraphs":[{"title":"Getting Started with Spark - Spark Workshop, Code4Lib 2018","text":"%md\nSpark Workshop, code4lib 2018\nMaterials at [github.com/spark4lib/code4lib2018](https://github.com/spark4lib/code4lib2018)\n\nThe following steps should also be available, with other notes, here: [https://github.com/spark4lib/code4lib2018/tree/master/worksheets/](https://github.com/spark4lib/code4lib2018/tree/master/worksheets) .","dateUpdated":"2018-02-12T01:17:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134174_-1435485370","id":"20180131-144721_1911570621","dateCreated":"2018-02-11T21:05:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:13030","user":"anonymous","dateFinished":"2018-02-12T01:17:06+0000","dateStarted":"2018-02-12T01:17:06+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Spark Workshop, code4lib 2018<br/>Materials at <a href=\"https://github.com/spark4lib/code4lib2018\">github.com/spark4lib/code4lib2018</a></p>\n<p>The following steps should also be available, with other notes, here: <a href=\"https://github.com/spark4lib/code4lib2018/tree/master/worksheets\">https://github.com/spark4lib/code4lib2018/tree/master/worksheets/</a> .</p>\n</div>"}]}},{"title":"Introduction","text":"%md\nIn this session, you will learn Zeppelin, Spark, & Spark SQL basics via primarily the DataFrames API. We are starting with a really simple dataset to focus on the tools. In the next session, we will use an expanded cultural heritage metadata set.\n\n#### Datasets? DataFrames?\n\nA **Dataset** is a distributed collection of data. Dataset provides the benefits of strong typing, ability to use powerful lambda functions with the benefits of (Spark SQL’s) optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java.\n\nA **DataFrame** is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. (Note that in Scala type parameters (generics) are enclosed in square brackets.)\n\n[[source](http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#datasets-and-dataframes)]\n\n#### How to run a paragraph\n\nTo run a paragraph in a Zeppelin notebook, you can either click the `play` button (blue triangle) on the right-hand side or click on the paragraph simply press `Shift + Enter`.\n\n#### What are Zeppelin Interpreters?\n\nIn the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with `%` followed by an interpreter name, e.g. `%spark2` for a Spark 2.x interpreter. Different interpreter names indicate what will be executed: code, markdown, html etc.  This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!\n\nThroughout this notebook we will use the following interpreters:\n\n- `` - the default interpreter is set to be PySpark for our Workshop Docker Container.\n- `%spark` - Spark interpreter to run Spark code written in Scala\n- `%spark.sql` - Spark SQL interprter (to execute SQL queries against temporary tables in Spark)\n- `%sh` - Shell interpreter to run shell commands\n- `%angular` - Angular interpreter to run Angular and HTML code\n- `%md` - Markdown for displaying formatted text, links, and images\n\nTo learn more about Zeppelin interpreters check out this [link](https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html).","dateUpdated":"2018-02-12T01:17:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134194_-1443180348","id":"20180211-041642_2058564180","dateCreated":"2018-02-11T21:05:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13031","user":"anonymous","dateFinished":"2018-02-12T01:17:20+0000","dateStarted":"2018-02-12T01:17:20+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this session, you will learn Zeppelin, Spark, &amp; Spark SQL basics via primarily the DataFrames API. We are starting with a really simple dataset to focus on the tools. In the next session, we will use an expanded cultural heritage metadata set.</p>\n<h4>Datasets? DataFrames?</h4>\n<p>A <strong>Dataset</strong> is a distributed collection of data. Dataset provides the benefits of strong typing, ability to use powerful lambda functions with the benefits of (Spark SQL’s) optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java.</p>\n<p>A <strong>DataFrame</strong> is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. (Note that in Scala type parameters (generics) are enclosed in square brackets.)</p>\n<p>[<a href=\"http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#datasets-and-dataframes\">source</a>]</p>\n<h4>How to run a paragraph</h4>\n<p>To run a paragraph in a Zeppelin notebook, you can either click the <code>play</code> button (blue triangle) on the right-hand side or click on the paragraph simply press <code>Shift + Enter</code>.</p>\n<h4>What are Zeppelin Interpreters?</h4>\n<p>In the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with <code>%</code> followed by an interpreter name, e.g. <code>%spark2</code> for a Spark 2.x interpreter. Different interpreter names indicate what will be executed: code, markdown, html etc. This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!</p>\n<p>Throughout this notebook we will use the following interpreters:</p>\n<ul>\n  <li>`` - the default interpreter is set to be PySpark for our Workshop Docker Container.</li>\n  <li><code>%spark</code> - Spark interpreter to run Spark code written in Scala</li>\n  <li><code>%spark.sql</code> - Spark SQL interprter (to execute SQL queries against temporary tables in Spark)</li>\n  <li><code>%sh</code> - Shell interpreter to run shell commands</li>\n  <li><code>%angular</code> - Angular interpreter to run Angular and HTML code</li>\n  <li><code>%md</code> - Markdown for displaying formatted text, links, and images</li>\n</ul>\n<p>To learn more about Zeppelin interpreters check out this <a href=\"https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html\">link</a>.</p>\n</div>"}]}},{"title":"Environment Set-up Test","text":"%spark\n\nprintln(\"Spark Version: \" + sc.version)\n\nval penndata = sc.textFile(\"penn.csv\")\nval cmoadata = sc.textFile(\"cmoa.csv\")\nprintln(\"penn count: \" + penndata.count)\nprintln(\"cmoadata count: \" + cmoadata.count)","dateUpdated":"2018-02-12T01:17:29+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134202_-1446258339","id":"20180131-144538_432654498","dateCreated":"2018-02-11T21:05:34+0000","status":"PENDING","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13032","user":"anonymous"},{"title":"Preview Sample Data","text":"%sh\ncat code4lib2018/sample-data/small-sample.csv","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134203_-1446643088","id":"20180210-010216_1168213332","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13033"},{"title":"Our Sample Data","text":"%md\n\nName | Institution Type | Format | URL | Description | Informal Score\n---- | ---------------- | ------ | --- | ----------- | --------------\nCMOA | Museum | \"JSON, CSV\" | https://github.com/cmoa/collection |  | 10\nPenn Museum | Museum | \"JSON, CSV, XML\" | https://www.penn.museum/collections/data.php | JSON is poorly structured | 7\nMet Museum | Museum | CSV |  | \"¯\\_(ツ)_/¯\n\" | 3\nDigitalNZ Te Puna Web Directory | Library | XML | https://natlib.govt.nz/files/data/tepunawebdirectory.xml | MARC XML | 3\nCanadian Subject Headings | Library | RDF/XML | http://www.collectionscanada.gc.ca/obj/900/f11/040004/csh.rdf | \"Ugh, rdf\" | 4\nDPLA | Aggregator  | \"CSV,JSON,XML\" | dp.la | | 100\n","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134203_-1446643088","id":"20180211-043610_1714070353","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13034"},{"title":"","text":"%md\n# Part 1: Creating a Dataframe Instance from CSV","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134203_-1446643088","id":"20180211-043612_1888524633","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13035"},{"title":"Read a CSV File into a Dataframe Instance & View It","text":"spark.read.csv(\"small-sample.csv\")\nspark.read.csv(\"small-sample.csv\").show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134204_-1448566833","id":"20180205-215000_1411168291","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13036"},{"title":"Read a CSV File into a Dataframe Instance with Headers","text":"spark.read.csv(\"code4lib2018/sample-data/small-sample.csv\", header=True).show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134204_-1448566833","id":"20180210-002750_1470779955","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13037"},{"title":"Read a CSV File into a Dataframe Instance with DROPMALFORMED mode","text":"spark.read.csv(\"code4lib2018/sample-data/small-sample.csv\", header=True, mode=\"DROPMALFORMED\").show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134205_-1448951582","id":"20180210-002834_476482959","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13038"},{"title":"Read a CSV File into a Dataframe Instance with multiLine...?","text":"spark.read.csv(\"code4lib2018/sample-data/small-sample.csv\", header=True, multiLine=True).show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134205_-1448951582","id":"20180210-002956_613592773","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13039"},{"title":"Temporary Python Repair of CSV Newlines within PySpark","text":"import csv\n\n# we will see a cleaner way using pyspark to handle this issue later\n# though really, use proper quoting with Spark 2.2.x & multifile=True\n\nwith open('code4lib2018/sample-data/small-sample.csv') as fh:\n    test = csv.reader(fh)\n    with open('code4lib2018/sample-data/small-sample-stripped.csv', 'w') as fout:\n        test_write = csv.writer(fout, quoting=csv.QUOTE_ALL)\n        for row in test:\n            new_row = [val.replace(\"\\r\\n\", \"\") for val in row]\n            test_write.writerow(new_row)","user":"anonymous","dateUpdated":"2018-02-11T21:08:27+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134205_-1448951582","id":"20180210-004851_951565379","dateCreated":"2018-02-11T21:05:34+0000","dateStarted":"2018-02-11T21:08:27+0000","dateFinished":"2018-02-11T21:08:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13040"},{"title":"Check that Our Temporary Repair Worked for our CSV","text":"%sh\ncat 'code4lib2018/sample-data/small-sample-stripped.csv'","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134206_-1447797335","id":"20180210-004958_2013545188","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13041"},{"title":"Revisit our CSV as a Dataframe & It's Inferred Schema","text":"spark.read.csv(\"code4lib2018/sample-data/small-sample-stripped.csv\", header=True, inferSchema=True).printSchema()","user":"anonymous","dateUpdated":"2018-02-11T21:08:32+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134206_-1447797335","id":"20180210-005045_657129235","dateCreated":"2018-02-11T21:05:34+0000","dateStarted":"2018-02-11T21:08:32+0000","dateFinished":"2018-02-11T21:08:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13042"},{"title":"Create Our Own Schema & Read the CSV into a Dataframe Instance with our Schema","text":"from pyspark.sql.types import *\n\ncustomSchema = StructType([\n  StructField(\"Name\", StringType(), True),\n  StructField(\"Institution Type\", StringType(), True),\n  StructField(\"Format\", StringType(), True),\n  StructField(\"URL\", StringType(), True),\n  StructField(\"Description\", StringType(), True),\n  StructField(\"Informal Score\", DecimalType(), True)])\n\nsampleDf = spark.read.csv(\"code4lib2018/sample-data/small-sample-stripped.csv\", header=True, schema=customSchema)\nsampleDf.show()","user":"anonymous","dateUpdated":"2018-02-11T21:08:37+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134206_-1447797335","id":"20180205-224139_1405421836","dateCreated":"2018-02-11T21:05:34+0000","dateStarted":"2018-02-11T21:08:37+0000","dateFinished":"2018-02-11T21:08:38+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13043"},{"title":"","text":"%md\n### Stopping Point: Have you been able to walk through and load your CSV?","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134207_-1448182084","id":"20180211-204040_1741647870","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13044"},{"title":"","text":"%md\n# Part 2: Simple Analysis of our Dataset via Dataframe API","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134207_-1448182084","id":"20180211-044238_542100528","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13045"},{"title":"Return First n Rows in your Dataframe ( .head(n) )","text":"sampleDf.head(2)","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134207_-1448182084","id":"20180210-203445_1454325544","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13046"},{"title":"Show Columns ( .columns() ) & Number of Columns ( .len() ) in your Dataframe","text":"print(sampleDf.columns)\nprint(len(sampleDf.columns))","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134208_-1757904949","id":"20180210-203724_1788119369","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13047"},{"title":"Show Specific Columns ( .select() )","text":"sampleDf.select(\"Name\").show()\nsampleDf.select(\"Name\", \"Informal Score\").show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134209_-1758289697","id":"20180211-044743_1705262796","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13048"},{"title":"Selecting Columns ( .select() ) versus Apply Filters ( .filter() )","text":"scoresBooleanDf = sampleDf.select('Name', 'URL', sampleDf['Informal Score'] > 5)\nscoresBooleanDf.show()\n\nhighScoresDf = sampleDf.filter(sampleDf['Informal Score'] > 5)\nhighScoresDf.select('Name', 'URL', 'Informal Score').show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134209_-1758289697","id":"20180210-011244_1927422767","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13049"},{"title":"How Many Rows? ( .count() )","text":"numSampleDf = sampleDf.count()\nnumHighScoresDf = highScoresDf.count()\n\nprint(\"Percentage of Datasets with a Score at or above 5: \" + str(float(numHighScoresDf)/float(numSampleDf)*100) + \"%\")","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134209_-1758289697","id":"20180205-225126_643532571","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13050"},{"title":"Show First n Rows of Dataframe in Show View ( .show(n) )","text":"sampleDf.show(2,truncate=True)","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134210_-1757135451","id":"20180210-203520_832539356","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13051"},{"title":"Show Stats Summary of your Dataframe ( .describe() )","text":"sampleDf.describe().show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134210_-1757135451","id":"20180210-203742_684138715","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13052"},{"title":"Show Stats Summary of a Specific Column ( .describe(colName) , DecimalType)","text":"sampleDf.describe('Informal Score').show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134211_-1757520200","id":"20180210-211801_243937278","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13053"},{"title":"Show Stats Summary of a Specific Column ( .describe(colName) , StringType)","text":"sampleDf.describe('URL').show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134211_-1757520200","id":"20180211-073524_2028466772","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13054"},{"title":"","text":"%md\n### Stopping Point: Have you been able to walk through reviewing some Rows & Columns?","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134212_-1759443944","id":"20180211-204350_627919107","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13055"},{"title":"Your Working Space: Analysis of Data I","text":"\n","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134212_-1759443944","id":"20180211-204610_53073346","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13056"},{"title":"Order Dataframe Views by a Column ( .orderBy(colName) | .orderBy(colName).desc() )","text":"sampleDf.orderBy(\"Informal Score\").show()\nsampleDf.orderBy(sampleDf[\"Informal Score\"].desc()).show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134213_-1759828693","id":"20180210-212920_1413184855","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13057"},{"title":"Group Values by a Column ( .groupBy(colName) ) ","text":"URLGroupDf = sampleDf.groupBy(\"Format\").count()\n\nURLGroupDf.show()\nURLGroupDf.printSchema()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134214_-1758674446","id":"20180210-011435_1074589917","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13058"},{"title":"Group Values by a Column then Compute an Aggregation ( .agg(expr) )","text":"# Note, this is setting us up for Splitting Values later on\n\nformatGroupDf = sampleDf.groupBy(\"Format\").agg({\"Informal Score\": 'mean'})\nformatGroupDf.show()\nformatGroupDf.printSchema()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134214_-1758674446","id":"20180211-053908_436020978","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13059"},{"title":"Show & Count Distinct Values in a Selected Column ( .distinct() )","text":"# Note, this is also setting us up for Splitting Values later on\n\ndistinctFormatDf = sampleDf.select(\"Format\").distinct()\ndistinctFormatDf.show()\ndistinctFormatDf.count()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134214_-1758674446","id":"20180210-211959_911280658","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13060"},{"title":"Use Aggregations ( .agg() ) & Sql Functions for Number of Distinct Values in Format ( countDistinct(colName) )","text":"from pyspark.sql.functions import countDistinct\n\ncountDistinctDF = sampleDf.select(\"Name\", \"Institution Type\", \"Format\").groupBy(\"Institution Type\").agg(countDistinct(\"Format\"))\ncountDistinctDF.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134215_-1759059195","id":"20180211-035942_1079861047","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13061"},{"title":"","text":"%md\n### Stopping Point: Have you been able to group, order, get distinct values by column(s)?","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134215_-1759059195","id":"20180211-204432_1327378340","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13062"},{"title":"Your Working Space: Analysis of Data II","text":"\n","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134216_-1760982940","id":"20180211-204542_862151313","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13063"},{"title":"","text":"%md\n# Part 3: Creating New Dataframes to Expand or Rework our Original Dataset","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134216_-1760982940","id":"20180211-052933_847596982","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13064"},{"title":"Create a Dataframe of DeDuplicated Values ( .dropDuplicates() )","text":"scoresDf = sampleDf.select('Informal Score').dropDuplicates()\nscoresDf.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134216_-1760982940","id":"20180210-212128_106635954","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13065"},{"title":"Create a Dataframe with Null Values Dropped ( .dropna() ) or Null Values Filled ( .fillna(fillerVal) )","text":"noNullDf = sampleDf.select('Name', 'URL').dropna()\nnoNullDf.show()\n\nnonNullDf = sampleDf.select('Name', 'URL').fillna('No URL')\nnonNullDf.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134217_-1761367689","id":"20180210-212150_723012495","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13066"},{"title":"Create Dataframe where URL is Non Null ( col.isNull() && .filter() )","text":"filterNonNullDF = sampleDf.filter(sampleDf.URL.isNull()).sort(\"Name\")\nfilterNonNullDF.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134217_-1761367689","id":"20180211-032722_1642010293","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13067"},{"title":"","text":"%md\n### Stopping Point: Create some Derivative Dataframes","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134217_-1761367689","id":"20180211-205119_924209624","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13068"},{"title":"Your Working Space: Deriving Data I","text":"\n","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134218_-1760213442","id":"20180211-204757_140350285","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13069"},{"title":"Create a new DataFrame with 'Format Array' column ( .withColumn(colName, colValue) ) of Split Values ( split(col, delimiter) )","text":"from pyspark.sql.functions import split\n\nsampleDf = sampleDf.withColumn(\"Format Array\", split(sampleDf.Format, \",\"))\nsampleDf.show()\nsampleDf.printSchema()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134218_-1760213442","id":"20180211-054754_1895175973","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13070"},{"title":"Use pyspark.sql.functions Explode ( explode(col) ) & Split ( .split(colName, delimiter) ) to Create a Row for each unique Format Value","text":"from pyspark.sql.functions import explode\n\nsampleDf = sampleDf.withColumn(\"Format\", explode(split(\"Format\", \",\")))\nsampleDf.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134219_-1760598191","id":"20180210-220422_1187559654","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13071"},{"title":"Create New Columns ( withColumn(colName, colValue) ) with Boolean if Format Type Present ( .like(Regex) )","text":"sampleDf.select(\"Format\").distinct().show()\nsampleDf = sampleDf.withColumn('CSV', sampleDf.Format.like(\"%CSV%\"))\nsampleDf = sampleDf.withColumn('XML', sampleDf.Format.like(\"%XML%\"))\nsampleDf = sampleDf.withColumn('RDF', sampleDf.Format.like(\"%RDF%\"))\nsampleDf = sampleDf.withColumn('JSON', sampleDf.Format.like(\"%JSON%\"))\nsampleDf.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{"0":{"graph":{"mode":"table","height":478,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134219_-1760598191","id":"20180210-213550_325903017","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13072"},{"title":"Transform our Dataframe to RDD ( .rdd ) & Map a function ( .map(lambda) )","text":"sampleRdd = sampleDf.select(\"Format\").rdd.map(lambda x: x[0].split(\",\"))\nsampleRdd.take(5)","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134229_-1753672711","id":"20180211-055309_116193468","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13073"},{"title":"Drop the Previous Format Array Column","text":"sampleDf = sampleDf.drop('Format Array')\nsampleDf.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134229_-1753672711","id":"20180210-213416_1825779467","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13074"},{"title":"Check our Distinct Format Values Now","text":"sampleDf.select(\"Format\").distinct().show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134229_-1752518464","id":"20180211-060909_1786306955","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13075"},{"title":"Remove Whitespace around Format using Regex ( reqexp_replace(colName, regex to replace, new value) )","text":"from pyspark.sql.functions import regexp_replace\n\nsampleDf = sampleDf.withColumn(\"Format\", regexp_replace(sampleDf.Format, \"\\s+\", \"\"))\nsampleDf.select(\"Format\").distinct().show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134231_-1752903213","id":"20180211-035409_418875897","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13076"},{"title":"Create Dataframe where Format is CSV ( .where(conditional) )","text":"CSVsampleDf = sampleDf.where((sampleDf.Format == \"CSV\"))\nCSVsampleDf.show()","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{"0":{"graph":{"mode":"table","height":181,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134233_-1755211706","id":"20180210-215449_392462777","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13077"},{"title":"","text":"%md\n### Stopping Point: Create some Derivative Dataframes to Address Multivalue Format or related Issues","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134234_-1754057460","id":"20180211-205149_2050933534","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13078"},{"title":"Your Working Space: Deriving Data II","text":"\n","dateUpdated":"2018-02-11T21:05:34+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134236_-1756365953","id":"20180211-205238_1685626655","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13079"},{"title":"","text":"%md\n# Part 4: Using SQL to Analyze our Dataset","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134236_-1756365953","id":"20180211-070011_2131671710","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13080"},{"title":"Introduction to SQL Queries ","text":"%md\nTo have a more dynamic experience, let’s create a temporary (in-memory) view that we can query against and interact with the resulting data in a table or graph format. The temporary view will allow us to execute SQL queries against it.\n\nNote that the temporary view will reside in memory as long as the Spark session is alive. [Here](http://cse.unl.edu/~sscott/ShowFiles/SQL/CheatSheet/SQLCheatSheet.html) is a SQL Cheatsheet in case you need it.","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134237_-1756750702","id":"20180211-040724_834116241","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13081"},{"title":"Creating a Temporary SQL View from our Sample Dataframe & Running a Simple SQL Query","text":"# Convert SampleDF DataFrame to a temporary view\nsampleDf.createOrReplaceTempView(\"sampleDataView\")\n\nsparkQuery = spark.sql(\"SELECT * FROM sampleDataView LIMIT 20\")\nsparkQuery.collect()","user":"anonymous","dateUpdated":"2018-02-11T21:08:42+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134237_-1756750702","id":"20180211-071952_1262010614","dateCreated":"2018-02-11T21:05:34+0000","dateStarted":"2018-02-11T21:08:42+0000","dateFinished":"2018-02-11T21:08:43+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13082"},{"title":"Querying our Temporary View for Number of Distinct Formats per Institution Type","text":"sparkQuery = spark.sql(\"\"\"SELECT `Institution Type`, COUNT(DISTINCT(Format)) AS NumFormats FROM sampleDataView GROUP BY `Institution Type`\"\"\")\nsparkQuery.collect()\nfor n in sparkQuery.collect():\n    n","user":"anonymous","dateUpdated":"2018-02-11T21:09:18+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134237_-1756750702","id":"20180211-072058_265727285","dateCreated":"2018-02-11T21:05:34+0000","dateStarted":"2018-02-11T21:09:18+0000","dateFinished":"2018-02-11T21:09:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13083"},{"title":"","text":"%md\n### Stopping Point: Create a Temporary View & Run a SQL Query","user":"anonymous","dateUpdated":"2018-02-11T21:11:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134238_-1755596455","id":"20180211-205631_117220376","dateCreated":"2018-02-11T21:05:34+0000","dateStarted":"2018-02-11T21:11:58+0000","dateFinished":"2018-02-11T21:11:58+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13084"},{"title":"Your Working Space: Create a Spark View from your Dataframe(s)","text":"\n","dateUpdated":"2018-02-11T21:11:20+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134238_-1755596455","id":"20180211-205653_111316135","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13085"},{"title":"Your Working Space: Run some Spark SQL Queries","text":"\n","user":"anonymous","dateUpdated":"2018-02-11T21:11:26+0000","config":{"lineNumbers":true,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383470603_883651304","id":"20180211-211110_718731458","dateCreated":"2018-02-11T21:11:10+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13086"},{"title":"","text":"%md\n# Part 5: Simple Data Visualization","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134238_-1755596455","id":"20180211-072644_1534169002","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13087"},{"text":"%sql \nSELECT `Institution Type`, COUNT(DISTINCT(Format)) AS NumFormats\nFROM sampleDataView\nGROUP BY `Institution Type`","user":"anonymous","dateUpdated":"2018-02-11T21:09:48+0000","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"scatterChart","height":300,"optionOpen":true},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383252281_-1241608030","id":"20180211-210732_864456388","dateCreated":"2018-02-11T21:07:32+0000","dateStarted":"2018-02-11T21:09:34+0000","dateFinished":"2018-02-11T21:09:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13088"},{"title":"","text":"%md\n### Stopping Point: From your Temporary Views, Create a Simple Viz","user":"anonymous","dateUpdated":"2018-02-11T21:12:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383521887_-2088457328","id":"20180211-211201_433698257","dateCreated":"2018-02-11T21:12:01+0000","dateStarted":"2018-02-11T21:12:16+0000","dateFinished":"2018-02-11T21:12:16+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13089"},{"title":"Your Working Space: Spark View => SQL Query & Viz","text":"%sql\n\n","user":"anonymous","dateUpdated":"2018-02-11T21:11:46+0000","config":{"lineNumbers":true,"editorSetting":{"language":"sql","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383429243_-836823183","id":"20180211-211029_1012657635","dateCreated":"2018-02-11T21:10:29+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13090"},{"text":"%md\n","dateUpdated":"2018-02-11T21:05:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1518383134239_-1755981204","id":"20180211-080404_990095708","dateCreated":"2018-02-11T21:05:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13091"}],"name":"Spark Workshop: Shell Intro","id":"2D63G57FZ","angularObjects":{"2D8F6TC39:shared_process":[],"2D7NXVY4B:shared_process":[],"2D8A6MQRC:shared_process":[],"2D5PJ3NSA:shared_process":[],"2D6EP7ZA2:shared_process":[],"2D5TJGRY4:shared_process":[],"2D5VZHKP9:shared_process":[],"2D89YMPRJ:shared_process":[],"2D72HVDPJ:shared_process":[],"2D6DB2GSE:shared_process":[],"2D7ZKW5Z8:shared_process":[],"2D6YB5W7T:shared_process":[],"2D6DKU8MK:shared_process":[],"2D4ZW7PCP:shared_process":[],"2D566ZBPK:shared_process":[],"2D866A3V3:shared_process":[],"2D7TTRTEG:shared_process":[],"2D71TQHNV:shared_process":[],"2D57SSSRY:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}